{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "homework-mlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZxw2lKep_yb"
      },
      "source": [
        "## Homework-X2: MLP for MNIST Classification\n",
        "\n",
        "### In this homework, you need to\n",
        "- #### implement SGD optimizer (`./optimizer.py`)\n",
        "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
        "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
        "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
        "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
        "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCIXDBTW7BQV"
      },
      "source": [
        "!git config --global user.email \"zgj17@mails.tsinghua.edu.cn\"\n",
        "!git config --global user.name \"molumitu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNMOeUxQqBZG",
        "outputId": "f22381d6-16c5-4d7f-dd4e-cb4498b2e2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!git clone https://molumitu:9865b0a4ee3f794113e2cbd2cf8aaf31c8425813@github.com/molumitu/hWX2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hWX2'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 38 (delta 17), reused 34 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwL_b_xh6mlS",
        "outputId": "93ae8b78-a990-410c-fdc1-1268139e7937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   criterion/softmax_cross_entropy.py\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqSB5vn06tV_"
      },
      "source": [
        "!git add -A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52_jaIij6yiy",
        "outputId": "06c079d4-e5cd-4059-c290-029179f0e282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!git commit -m \"completed！\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master e932f9e] completed！\n",
            " 1 file changed, 19 insertions(+), 9 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLlzJz4a7UzS",
        "outputId": "6c133772-ed77-4a4c-dc51-b86266bc5322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!git push"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 4, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  25% (1/4)   \rCompressing objects:  50% (2/4)   \rCompressing objects:  75% (3/4)   \rCompressing objects: 100% (4/4)   \rCompressing objects: 100% (4/4), done.\n",
            "Writing objects:  25% (1/4)   \rWriting objects:  50% (2/4)   \rWriting objects:  75% (3/4)   \rWriting objects: 100% (4/4)   \rWriting objects: 100% (4/4), 601 bytes | 601.00 KiB/s, done.\n",
            "Total 4 (delta 3), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
            "To https://github.com/molumitu/hWX2\n",
            "   8798a68..e932f9e  master -> master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxoAiw38txh0",
        "outputId": "cd5faf3e-d3f2-4eac-a3d1-d088ead56c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/hWX2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/hWX2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ayi1choyGxm"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JK1o55Op_yc",
        "outputId": "a9be9553-99e3-425e-b3cd-bc6028ec32af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from network import Network\n",
        "from solver import train, test\n",
        "from plot import plot_loss_and_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTU5sDZTp_ye"
      },
      "source": [
        "## Load MNIST Dataset\n",
        "We use tensorflow tools to load dataset for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmy3ANiXp_ye"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQLUlkbop_yg"
      },
      "source": [
        "def decode_image(image):\n",
        "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.reshape(image, [784])\n",
        "    image = image / 255.0\n",
        "    image = image - tf.reduce_mean(image)\n",
        "    return image\n",
        "\n",
        "def decode_label(label):\n",
        "    # Encode label with one-hot encoding\n",
        "    return tf.one_hot(label, depth=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLSUOWSap_yi"
      },
      "source": [
        "# Data Preprocessing\n",
        "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
        "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
        "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
        "\n",
        "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
        "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
        "data_test = tf.data.Dataset.zip((x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iJqA-I-p_yl"
      },
      "source": [
        "## Set Hyerparameters\n",
        "You can modify hyerparameters by yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7eG767Mp_yl"
      },
      "source": [
        "batch_size = 100\n",
        "max_epoch = 20\n",
        "init_std = 0.01\n",
        "\n",
        "learning_rate_SGD = 0.01\n",
        "weight_decay = 0\n",
        "\n",
        "disp_freq = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F76MBqtJp_yn"
      },
      "source": [
        "## 1. MLP with Euclidean Loss\n",
        "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
        "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
        "### TODO\n",
        "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzLCgNKAp_yn"
      },
      "source": [
        "from criterion import EuclideanLossLayer\n",
        "from optimizer import SGD\n",
        "\n",
        "criterion = EuclideanLossLayer()\n",
        "\n",
        "sgd = SGD(learning_rate_SGD, weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ItIABFp_yq"
      },
      "source": [
        "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
        "\n",
        "### TODO\n",
        "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjnviAVwp_yq"
      },
      "source": [
        "from layers import FCLayer, SigmoidLayer\n",
        "\n",
        "sigmoidMLP = Network()\n",
        "# Build MLP with FCLayer and SigmoidLayer\n",
        "# 128 is the number of hidden units, you can change by your own\n",
        "sigmoidMLP.add(FCLayer(784, 128))\n",
        "sigmoidMLP.add(SigmoidLayer())\n",
        "sigmoidMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hVSk9a_bp_ys",
        "outputId": "515c7bdd-20cd-409e-cbc7-1c298d23f029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/hWX2/solver.py:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8700\t Accuracy 0.1300\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8710\t Accuracy 0.1290\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8604\t Accuracy 0.1396\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8528\t Accuracy 0.1472\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8437\t Accuracy 0.1563\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8342\t Accuracy 0.1658\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8235\t Accuracy 0.1765\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8156\t Accuracy 0.1844\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8077\t Accuracy 0.1923\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7999\t Accuracy 0.2001\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7905\t Accuracy 0.2095\n",
            "\n",
            "Epoch [0]\t Average training loss 0.7821\t Average training accuracy 0.2179\n",
            "Epoch [0]\t Average validation loss 0.6688\t Average validation accuracy 0.3312\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.6900\t Accuracy 0.3100\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.6794\t Accuracy 0.3206\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.6739\t Accuracy 0.3261\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.6677\t Accuracy 0.3323\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.6610\t Accuracy 0.3390\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.6522\t Accuracy 0.3478\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.6445\t Accuracy 0.3555\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.6407\t Accuracy 0.3593\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.6362\t Accuracy 0.3638\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.6307\t Accuracy 0.3693\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.6244\t Accuracy 0.3756\n",
            "\n",
            "Epoch [1]\t Average training loss 0.6186\t Average training accuracy 0.3814\n",
            "Epoch [1]\t Average validation loss 0.5244\t Average validation accuracy 0.4756\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.5100\t Accuracy 0.4900\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.5463\t Accuracy 0.4537\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.5459\t Accuracy 0.4541\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.5466\t Accuracy 0.4534\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.5420\t Accuracy 0.4580\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.5372\t Accuracy 0.4628\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.5320\t Accuracy 0.4680\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.5316\t Accuracy 0.4684\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.5291\t Accuracy 0.4709\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.5252\t Accuracy 0.4748\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.5217\t Accuracy 0.4783\n",
            "\n",
            "Epoch [2]\t Average training loss 0.5179\t Average training accuracy 0.4821\n",
            "Epoch [2]\t Average validation loss 0.4486\t Average validation accuracy 0.5514\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.4400\t Accuracy 0.5600\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.4661\t Accuracy 0.5339\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.4694\t Accuracy 0.5306\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.4739\t Accuracy 0.5261\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.4717\t Accuracy 0.5283\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.4686\t Accuracy 0.5314\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.4645\t Accuracy 0.5355\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.4654\t Accuracy 0.5346\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.4643\t Accuracy 0.5357\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.4614\t Accuracy 0.5386\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.4592\t Accuracy 0.5408\n",
            "\n",
            "Epoch [3]\t Average training loss 0.4560\t Average training accuracy 0.5440\n",
            "Epoch [3]\t Average validation loss 0.3962\t Average validation accuracy 0.6038\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3700\t Accuracy 0.6300\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.4161\t Accuracy 0.5839\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.4194\t Accuracy 0.5806\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.4250\t Accuracy 0.5750\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.4241\t Accuracy 0.5759\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.4227\t Accuracy 0.5773\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.4203\t Accuracy 0.5797\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.4215\t Accuracy 0.5785\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.4207\t Accuracy 0.5793\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.4193\t Accuracy 0.5807\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.4177\t Accuracy 0.5823\n",
            "\n",
            "Epoch [4]\t Average training loss 0.4148\t Average training accuracy 0.5852\n",
            "Epoch [4]\t Average validation loss 0.3584\t Average validation accuracy 0.6416\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.3500\t Accuracy 0.6500\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.3845\t Accuracy 0.6155\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3882\t Accuracy 0.6118\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3938\t Accuracy 0.6062\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3925\t Accuracy 0.6075\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3908\t Accuracy 0.6092\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3897\t Accuracy 0.6103\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3909\t Accuracy 0.6091\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3908\t Accuracy 0.6092\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3901\t Accuracy 0.6099\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3888\t Accuracy 0.6112\n",
            "\n",
            "Epoch [5]\t Average training loss 0.3861\t Average training accuracy 0.6139\n",
            "Epoch [5]\t Average validation loss 0.3376\t Average validation accuracy 0.6624\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.3300\t Accuracy 0.6700\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.3625\t Accuracy 0.6375\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.3641\t Accuracy 0.6359\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3695\t Accuracy 0.6305\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3691\t Accuracy 0.6309\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3670\t Accuracy 0.6330\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3661\t Accuracy 0.6339\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3678\t Accuracy 0.6322\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3675\t Accuracy 0.6325\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3670\t Accuracy 0.6330\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3661\t Accuracy 0.6339\n",
            "\n",
            "Epoch [6]\t Average training loss 0.3634\t Average training accuracy 0.6366\n",
            "Epoch [6]\t Average validation loss 0.3146\t Average validation accuracy 0.6854\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2900\t Accuracy 0.7100\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.3433\t Accuracy 0.6567\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.3461\t Accuracy 0.6539\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.3509\t Accuracy 0.6491\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.3505\t Accuracy 0.6495\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.3487\t Accuracy 0.6513\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.3479\t Accuracy 0.6521\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.3496\t Accuracy 0.6504\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.3492\t Accuracy 0.6508\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.3491\t Accuracy 0.6509\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.3486\t Accuracy 0.6514\n",
            "\n",
            "Epoch [7]\t Average training loss 0.3461\t Average training accuracy 0.6539\n",
            "Epoch [7]\t Average validation loss 0.3006\t Average validation accuracy 0.6994\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2700\t Accuracy 0.7300\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.3247\t Accuracy 0.6753\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.3292\t Accuracy 0.6708\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.3343\t Accuracy 0.6657\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.3340\t Accuracy 0.6660\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.3326\t Accuracy 0.6674\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.3321\t Accuracy 0.6679\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.3340\t Accuracy 0.6660\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.3340\t Accuracy 0.6660\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.3343\t Accuracy 0.6657\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.3338\t Accuracy 0.6662\n",
            "\n",
            "Epoch [8]\t Average training loss 0.3314\t Average training accuracy 0.6686\n",
            "Epoch [8]\t Average validation loss 0.2872\t Average validation accuracy 0.7128\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2700\t Accuracy 0.7300\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.3127\t Accuracy 0.6873\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.3168\t Accuracy 0.6832\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.3218\t Accuracy 0.6782\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.3214\t Accuracy 0.6786\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.3202\t Accuracy 0.6798\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.3201\t Accuracy 0.6799\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.3217\t Accuracy 0.6783\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.3219\t Accuracy 0.6781\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.3219\t Accuracy 0.6781\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.3218\t Accuracy 0.6782\n",
            "\n",
            "Epoch [9]\t Average training loss 0.3194\t Average training accuracy 0.6806\n",
            "Epoch [9]\t Average validation loss 0.2740\t Average validation accuracy 0.7260\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2500\t Accuracy 0.7500\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.3018\t Accuracy 0.6982\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.3070\t Accuracy 0.6930\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.3118\t Accuracy 0.6882\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.3107\t Accuracy 0.6893\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.3100\t Accuracy 0.6900\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.3098\t Accuracy 0.6902\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.3113\t Accuracy 0.6887\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.3110\t Accuracy 0.6890\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.3113\t Accuracy 0.6887\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.3115\t Accuracy 0.6885\n",
            "\n",
            "Epoch [10]\t Average training loss 0.3093\t Average training accuracy 0.6907\n",
            "Epoch [10]\t Average validation loss 0.2674\t Average validation accuracy 0.7326\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2300\t Accuracy 0.7700\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2904\t Accuracy 0.7096\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2956\t Accuracy 0.7044\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.3009\t Accuracy 0.6991\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.3003\t Accuracy 0.6997\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.3003\t Accuracy 0.6997\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.3003\t Accuracy 0.6997\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.3017\t Accuracy 0.6983\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.3016\t Accuracy 0.6984\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.3020\t Accuracy 0.6980\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.3021\t Accuracy 0.6979\n",
            "\n",
            "Epoch [11]\t Average training loss 0.2998\t Average training accuracy 0.7002\n",
            "Epoch [11]\t Average validation loss 0.2590\t Average validation accuracy 0.7410\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2200\t Accuracy 0.7800\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2806\t Accuracy 0.7194\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2853\t Accuracy 0.7147\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2921\t Accuracy 0.7079\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2914\t Accuracy 0.7086\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2916\t Accuracy 0.7084\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2920\t Accuracy 0.7080\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2933\t Accuracy 0.7067\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2934\t Accuracy 0.7066\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2939\t Accuracy 0.7061\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2942\t Accuracy 0.7058\n",
            "\n",
            "Epoch [12]\t Average training loss 0.2923\t Average training accuracy 0.7077\n",
            "Epoch [12]\t Average validation loss 0.2516\t Average validation accuracy 0.7484\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2200\t Accuracy 0.7800\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2718\t Accuracy 0.7282\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2781\t Accuracy 0.7219\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2846\t Accuracy 0.7154\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2841\t Accuracy 0.7159\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2845\t Accuracy 0.7155\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2845\t Accuracy 0.7155\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2854\t Accuracy 0.7146\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2855\t Accuracy 0.7145\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2862\t Accuracy 0.7138\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2866\t Accuracy 0.7134\n",
            "\n",
            "Epoch [13]\t Average training loss 0.2848\t Average training accuracy 0.7152\n",
            "Epoch [13]\t Average validation loss 0.2450\t Average validation accuracy 0.7550\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2200\t Accuracy 0.7800\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2665\t Accuracy 0.7335\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2735\t Accuracy 0.7265\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2793\t Accuracy 0.7207\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2783\t Accuracy 0.7217\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2779\t Accuracy 0.7221\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2778\t Accuracy 0.7222\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2791\t Accuracy 0.7209\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2791\t Accuracy 0.7209\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2798\t Accuracy 0.7202\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2802\t Accuracy 0.7198\n",
            "\n",
            "Epoch [14]\t Average training loss 0.2783\t Average training accuracy 0.7217\n",
            "Epoch [14]\t Average validation loss 0.2388\t Average validation accuracy 0.7612\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2200\t Accuracy 0.7800\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2625\t Accuracy 0.7375\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2692\t Accuracy 0.7308\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2746\t Accuracy 0.7254\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2728\t Accuracy 0.7272\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2727\t Accuracy 0.7273\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2723\t Accuracy 0.7277\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2736\t Accuracy 0.7264\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2736\t Accuracy 0.7264\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2742\t Accuracy 0.7258\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2747\t Accuracy 0.7253\n",
            "\n",
            "Epoch [15]\t Average training loss 0.2730\t Average training accuracy 0.7270\n",
            "Epoch [15]\t Average validation loss 0.2336\t Average validation accuracy 0.7664\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2100\t Accuracy 0.7900\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2573\t Accuracy 0.7427\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2633\t Accuracy 0.7367\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2687\t Accuracy 0.7313\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2669\t Accuracy 0.7331\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2668\t Accuracy 0.7332\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2664\t Accuracy 0.7336\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2681\t Accuracy 0.7319\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2681\t Accuracy 0.7319\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2688\t Accuracy 0.7312\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2696\t Accuracy 0.7304\n",
            "\n",
            "Epoch [16]\t Average training loss 0.2679\t Average training accuracy 0.7321\n",
            "Epoch [16]\t Average validation loss 0.2296\t Average validation accuracy 0.7704\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2100\t Accuracy 0.7900\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2514\t Accuracy 0.7486\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2581\t Accuracy 0.7419\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2630\t Accuracy 0.7370\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2614\t Accuracy 0.7386\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2615\t Accuracy 0.7385\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2612\t Accuracy 0.7388\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2631\t Accuracy 0.7369\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2630\t Accuracy 0.7370\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2638\t Accuracy 0.7362\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2648\t Accuracy 0.7352\n",
            "\n",
            "Epoch [17]\t Average training loss 0.2630\t Average training accuracy 0.7370\n",
            "Epoch [17]\t Average validation loss 0.2244\t Average validation accuracy 0.7756\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1900\t Accuracy 0.8100\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2480\t Accuracy 0.7520\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2546\t Accuracy 0.7454\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2597\t Accuracy 0.7403\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2580\t Accuracy 0.7420\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2581\t Accuracy 0.7419\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2577\t Accuracy 0.7423\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2596\t Accuracy 0.7404\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2596\t Accuracy 0.7404\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2605\t Accuracy 0.7395\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2614\t Accuracy 0.7386\n",
            "\n",
            "Epoch [18]\t Average training loss 0.2597\t Average training accuracy 0.7403\n",
            "Epoch [18]\t Average validation loss 0.2208\t Average validation accuracy 0.7792\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1900\t Accuracy 0.8100\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2443\t Accuracy 0.7557\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2513\t Accuracy 0.7487\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2557\t Accuracy 0.7443\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2537\t Accuracy 0.7463\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2538\t Accuracy 0.7462\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2533\t Accuracy 0.7467\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2552\t Accuracy 0.7448\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2553\t Accuracy 0.7447\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2561\t Accuracy 0.7439\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2572\t Accuracy 0.7428\n",
            "\n",
            "Epoch [19]\t Average training loss 0.2555\t Average training accuracy 0.7445\n",
            "Epoch [19]\t Average validation loss 0.2174\t Average validation accuracy 0.7826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRbVyaQGp_yu",
        "outputId": "dd2dd45a-2c6c-468b-d949-47150303e936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.7578.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_QqAe6Sp_yv"
      },
      "source": [
        "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
        "\n",
        "### TODO\n",
        "Before executing the following code, you should complete **layers/relu_layer.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgii8iN6p_yw"
      },
      "source": [
        "from layers import ReLULayer\n",
        "\n",
        "reluMLP = Network()\n",
        "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
        "reluMLP.add(FCLayer(784, 128))\n",
        "reluMLP.add(ReLULayer())\n",
        "reluMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SjSbUpuOp_yy",
        "outputId": "54958541-5f02-4e9a-92e0-2f54b73b8ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.9400\t Accuracy 0.0600\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8771\t Accuracy 0.1229\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8478\t Accuracy 0.1522\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8224\t Accuracy 0.1776\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8013\t Accuracy 0.1987\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.7808\t Accuracy 0.2192\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.7594\t Accuracy 0.2406\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.7430\t Accuracy 0.2570\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.7251\t Accuracy 0.2749\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7095\t Accuracy 0.2905\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.6952\t Accuracy 0.3048\n",
            "\n",
            "Epoch [0]\t Average training loss 0.6783\t Average training accuracy 0.3217\n",
            "Epoch [0]\t Average validation loss 0.4750\t Average validation accuracy 0.5250\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4900\t Accuracy 0.5100\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4831\t Accuracy 0.5169\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4828\t Accuracy 0.5172\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4763\t Accuracy 0.5237\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4720\t Accuracy 0.5280\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4657\t Accuracy 0.5343\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4583\t Accuracy 0.5417\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4552\t Accuracy 0.5448\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4500\t Accuracy 0.5500\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4472\t Accuracy 0.5528\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4442\t Accuracy 0.5558\n",
            "\n",
            "Epoch [1]\t Average training loss 0.4375\t Average training accuracy 0.5625\n",
            "Epoch [1]\t Average validation loss 0.3494\t Average validation accuracy 0.6506\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3700\t Accuracy 0.6300\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3669\t Accuracy 0.6331\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3701\t Accuracy 0.6299\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3692\t Accuracy 0.6308\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3681\t Accuracy 0.6319\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3657\t Accuracy 0.6343\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3622\t Accuracy 0.6378\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3619\t Accuracy 0.6381\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3601\t Accuracy 0.6399\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3596\t Accuracy 0.6404\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3590\t Accuracy 0.6410\n",
            "\n",
            "Epoch [2]\t Average training loss 0.3552\t Average training accuracy 0.6448\n",
            "Epoch [2]\t Average validation loss 0.2954\t Average validation accuracy 0.7046\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2500\t Accuracy 0.7500\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3163\t Accuracy 0.6837\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3197\t Accuracy 0.6803\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3191\t Accuracy 0.6809\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3187\t Accuracy 0.6813\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3168\t Accuracy 0.6832\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3142\t Accuracy 0.6858\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3154\t Accuracy 0.6846\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3142\t Accuracy 0.6858\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3145\t Accuracy 0.6855\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3147\t Accuracy 0.6853\n",
            "\n",
            "Epoch [3]\t Average training loss 0.3120\t Average training accuracy 0.6880\n",
            "Epoch [3]\t Average validation loss 0.2588\t Average validation accuracy 0.7412\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2400\t Accuracy 0.7600\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2833\t Accuracy 0.7167\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2886\t Accuracy 0.7114\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2876\t Accuracy 0.7124\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2874\t Accuracy 0.7126\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2859\t Accuracy 0.7141\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2838\t Accuracy 0.7162\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2857\t Accuracy 0.7143\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2853\t Accuracy 0.7147\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2863\t Accuracy 0.7137\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2871\t Accuracy 0.7129\n",
            "\n",
            "Epoch [4]\t Average training loss 0.2847\t Average training accuracy 0.7153\n",
            "Epoch [4]\t Average validation loss 0.2354\t Average validation accuracy 0.7646\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2200\t Accuracy 0.7800\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2624\t Accuracy 0.7376\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2674\t Accuracy 0.7326\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2662\t Accuracy 0.7338\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2662\t Accuracy 0.7338\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2649\t Accuracy 0.7351\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2632\t Accuracy 0.7368\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2654\t Accuracy 0.7346\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2651\t Accuracy 0.7349\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2661\t Accuracy 0.7339\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2670\t Accuracy 0.7330\n",
            "\n",
            "Epoch [5]\t Average training loss 0.2651\t Average training accuracy 0.7349\n",
            "Epoch [5]\t Average validation loss 0.2160\t Average validation accuracy 0.7840\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2300\t Accuracy 0.7700\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2431\t Accuracy 0.7569\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2496\t Accuracy 0.7504\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2491\t Accuracy 0.7509\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2503\t Accuracy 0.7497\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2486\t Accuracy 0.7514\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2481\t Accuracy 0.7519\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2500\t Accuracy 0.7500\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2498\t Accuracy 0.7502\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2510\t Accuracy 0.7490\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2521\t Accuracy 0.7479\n",
            "\n",
            "Epoch [6]\t Average training loss 0.2504\t Average training accuracy 0.7496\n",
            "Epoch [6]\t Average validation loss 0.2062\t Average validation accuracy 0.7938\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2000\t Accuracy 0.8000\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2290\t Accuracy 0.7710\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2357\t Accuracy 0.7643\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2368\t Accuracy 0.7632\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2379\t Accuracy 0.7621\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2355\t Accuracy 0.7645\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2352\t Accuracy 0.7648\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2379\t Accuracy 0.7621\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2380\t Accuracy 0.7620\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2392\t Accuracy 0.7608\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2406\t Accuracy 0.7594\n",
            "\n",
            "Epoch [7]\t Average training loss 0.2389\t Average training accuracy 0.7611\n",
            "Epoch [7]\t Average validation loss 0.1962\t Average validation accuracy 0.8038\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2000\t Accuracy 0.8000\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2182\t Accuracy 0.7818\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2252\t Accuracy 0.7748\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2264\t Accuracy 0.7736\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2280\t Accuracy 0.7720\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2252\t Accuracy 0.7748\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2254\t Accuracy 0.7746\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2282\t Accuracy 0.7718\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2286\t Accuracy 0.7714\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2301\t Accuracy 0.7699\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2318\t Accuracy 0.7682\n",
            "\n",
            "Epoch [8]\t Average training loss 0.2303\t Average training accuracy 0.7697\n",
            "Epoch [8]\t Average validation loss 0.1894\t Average validation accuracy 0.8106\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2000\t Accuracy 0.8000\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2114\t Accuracy 0.7886\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2184\t Accuracy 0.7816\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2193\t Accuracy 0.7807\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2204\t Accuracy 0.7796\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2175\t Accuracy 0.7825\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2181\t Accuracy 0.7819\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2205\t Accuracy 0.7795\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2208\t Accuracy 0.7792\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2225\t Accuracy 0.7775\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2241\t Accuracy 0.7759\n",
            "\n",
            "Epoch [9]\t Average training loss 0.2228\t Average training accuracy 0.7772\n",
            "Epoch [9]\t Average validation loss 0.1828\t Average validation accuracy 0.8172\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2000\t Accuracy 0.8000\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2041\t Accuracy 0.7959\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2098\t Accuracy 0.7902\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2107\t Accuracy 0.7893\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2121\t Accuracy 0.7879\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2099\t Accuracy 0.7901\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2106\t Accuracy 0.7894\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2130\t Accuracy 0.7870\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2135\t Accuracy 0.7865\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2153\t Accuracy 0.7847\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2168\t Accuracy 0.7832\n",
            "\n",
            "Epoch [10]\t Average training loss 0.2158\t Average training accuracy 0.7842\n",
            "Epoch [10]\t Average validation loss 0.1780\t Average validation accuracy 0.8220\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1900\t Accuracy 0.8100\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1969\t Accuracy 0.8031\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2029\t Accuracy 0.7971\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2043\t Accuracy 0.7957\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2058\t Accuracy 0.7942\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2036\t Accuracy 0.7964\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2042\t Accuracy 0.7958\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2069\t Accuracy 0.7931\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2077\t Accuracy 0.7923\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2096\t Accuracy 0.7904\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2109\t Accuracy 0.7891\n",
            "\n",
            "Epoch [11]\t Average training loss 0.2099\t Average training accuracy 0.7901\n",
            "Epoch [11]\t Average validation loss 0.1714\t Average validation accuracy 0.8286\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1700\t Accuracy 0.8300\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1916\t Accuracy 0.8084\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1979\t Accuracy 0.8021\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1999\t Accuracy 0.8001\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2011\t Accuracy 0.7989\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1986\t Accuracy 0.8014\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1993\t Accuracy 0.8007\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2019\t Accuracy 0.7981\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2027\t Accuracy 0.7973\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2045\t Accuracy 0.7955\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2057\t Accuracy 0.7943\n",
            "\n",
            "Epoch [12]\t Average training loss 0.2048\t Average training accuracy 0.7952\n",
            "Epoch [12]\t Average validation loss 0.1696\t Average validation accuracy 0.8304\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1600\t Accuracy 0.8400\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1865\t Accuracy 0.8135\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1941\t Accuracy 0.8059\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1964\t Accuracy 0.8036\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1973\t Accuracy 0.8027\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1947\t Accuracy 0.8053\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1953\t Accuracy 0.8047\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1979\t Accuracy 0.8021\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1988\t Accuracy 0.8012\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2006\t Accuracy 0.7994\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2018\t Accuracy 0.7982\n",
            "\n",
            "Epoch [13]\t Average training loss 0.2010\t Average training accuracy 0.7990\n",
            "Epoch [13]\t Average validation loss 0.1678\t Average validation accuracy 0.8322\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1600\t Accuracy 0.8400\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1841\t Accuracy 0.8159\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1908\t Accuracy 0.8092\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1933\t Accuracy 0.8067\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1941\t Accuracy 0.8059\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1911\t Accuracy 0.8089\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1920\t Accuracy 0.8080\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1946\t Accuracy 0.8054\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1952\t Accuracy 0.8048\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1970\t Accuracy 0.8030\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1985\t Accuracy 0.8015\n",
            "\n",
            "Epoch [14]\t Average training loss 0.1977\t Average training accuracy 0.8023\n",
            "Epoch [14]\t Average validation loss 0.1646\t Average validation accuracy 0.8354\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1600\t Accuracy 0.8400\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1816\t Accuracy 0.8184\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1873\t Accuracy 0.8127\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1903\t Accuracy 0.8097\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1907\t Accuracy 0.8093\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1876\t Accuracy 0.8124\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1886\t Accuracy 0.8114\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1912\t Accuracy 0.8088\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1918\t Accuracy 0.8082\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1936\t Accuracy 0.8064\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1950\t Accuracy 0.8050\n",
            "\n",
            "Epoch [15]\t Average training loss 0.1943\t Average training accuracy 0.8057\n",
            "Epoch [15]\t Average validation loss 0.1612\t Average validation accuracy 0.8388\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1700\t Accuracy 0.8300\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1778\t Accuracy 0.8222\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1832\t Accuracy 0.8168\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1866\t Accuracy 0.8134\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1870\t Accuracy 0.8130\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1842\t Accuracy 0.8158\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1853\t Accuracy 0.8147\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1879\t Accuracy 0.8121\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1886\t Accuracy 0.8114\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1902\t Accuracy 0.8098\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1916\t Accuracy 0.8084\n",
            "\n",
            "Epoch [16]\t Average training loss 0.1910\t Average training accuracy 0.8090\n",
            "Epoch [16]\t Average validation loss 0.1588\t Average validation accuracy 0.8412\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1700\t Accuracy 0.8300\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1747\t Accuracy 0.8253\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1801\t Accuracy 0.8199\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1835\t Accuracy 0.8165\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1845\t Accuracy 0.8155\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1816\t Accuracy 0.8184\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1827\t Accuracy 0.8173\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1852\t Accuracy 0.8148\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1859\t Accuracy 0.8141\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1875\t Accuracy 0.8125\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1889\t Accuracy 0.8111\n",
            "\n",
            "Epoch [17]\t Average training loss 0.1883\t Average training accuracy 0.8117\n",
            "Epoch [17]\t Average validation loss 0.1570\t Average validation accuracy 0.8430\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1800\t Accuracy 0.8200\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1724\t Accuracy 0.8276\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1777\t Accuracy 0.8223\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1814\t Accuracy 0.8186\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1816\t Accuracy 0.8184\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1788\t Accuracy 0.8212\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1797\t Accuracy 0.8203\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1822\t Accuracy 0.8178\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1830\t Accuracy 0.8170\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1845\t Accuracy 0.8155\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1859\t Accuracy 0.8141\n",
            "\n",
            "Epoch [18]\t Average training loss 0.1855\t Average training accuracy 0.8145\n",
            "Epoch [18]\t Average validation loss 0.1558\t Average validation accuracy 0.8442\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1700\t Accuracy 0.8300\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1690\t Accuracy 0.8310\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1749\t Accuracy 0.8251\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1782\t Accuracy 0.8218\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1788\t Accuracy 0.8212\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1762\t Accuracy 0.8238\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1769\t Accuracy 0.8231\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1795\t Accuracy 0.8205\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1804\t Accuracy 0.8196\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1820\t Accuracy 0.8180\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1833\t Accuracy 0.8167\n",
            "\n",
            "Epoch [19]\t Average training loss 0.1829\t Average training accuracy 0.8171\n",
            "Epoch [19]\t Average validation loss 0.1540\t Average validation accuracy 0.8460\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnR0wmJ-p_yz",
        "outputId": "bfe092af-30c9-4d65-caf5-6d1c4f568228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.8274.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SikJXIV6p_y2"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmkJpnCHp_y2",
        "outputId": "db11d6c4-df2a-48d8-8769-cc772cd4370e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
        "                   'relu': [relu_loss, relu_acc]})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTb_rY8xp_y4"
      },
      "source": [
        "## 2. MLP with Softmax Cross-Entropy Loss\n",
        "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
        "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
        "### TODO\n",
        "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAPpL5fbp_y4"
      },
      "source": [
        "from criterion import SoftmaxCrossEntropyLossLayer\n",
        "\n",
        "criterion = SoftmaxCrossEntropyLossLayer()\n",
        "\n",
        "sgd = SGD(learning_rate_SGD, weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o51jX7DYp_y6"
      },
      "source": [
        "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QXtnVCFp_y6"
      },
      "source": [
        "sigmoidMLP = Network()\n",
        "# Build MLP with FCLayer and SigmoidLayer\n",
        "# 128 is the number of hidden units, you can change by your own\n",
        "sigmoidMLP.add(FCLayer(784, 128))\n",
        "sigmoidMLP.add(SigmoidLayer())\n",
        "sigmoidMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "empCMjRRp_y8"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h-0TrPgsp_y8",
        "outputId": "95ed59c8-5c17-4238-c47b-14ad4eb23d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7683\t Accuracy 0.1100\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4769\t Accuracy 0.0871\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4029\t Accuracy 0.0895\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3697\t Accuracy 0.0945\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3497\t Accuracy 0.0967\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3353\t Accuracy 0.0998\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.3227\t Accuracy 0.1052\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.3121\t Accuracy 0.1124\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.3020\t Accuracy 0.1204\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2932\t Accuracy 0.1282\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.2853\t Accuracy 0.1352\n",
            "\n",
            "Epoch [0]\t Average training loss 2.2775\t Average training accuracy 0.1415\n",
            "Epoch [0]\t Average validation loss 2.1866\t Average validation accuracy 0.2284\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.1858\t Accuracy 0.2400\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.1795\t Accuracy 0.2412\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.1755\t Accuracy 0.2419\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.1694\t Accuracy 0.2480\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.1642\t Accuracy 0.2557\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.1587\t Accuracy 0.2638\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1527\t Accuracy 0.2743\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1482\t Accuracy 0.2829\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1423\t Accuracy 0.2937\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.1377\t Accuracy 0.3027\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.1330\t Accuracy 0.3112\n",
            "\n",
            "Epoch [1]\t Average training loss 2.1280\t Average training accuracy 0.3196\n",
            "Epoch [1]\t Average validation loss 2.0617\t Average validation accuracy 0.4392\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.0594\t Accuracy 0.4900\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.0608\t Accuracy 0.4314\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.0581\t Accuracy 0.4266\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.0542\t Accuracy 0.4250\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.0506\t Accuracy 0.4286\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.0463\t Accuracy 0.4350\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.0414\t Accuracy 0.4401\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.0389\t Accuracy 0.4416\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0341\t Accuracy 0.4495\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0310\t Accuracy 0.4532\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0275\t Accuracy 0.4583\n",
            "\n",
            "Epoch [2]\t Average training loss 2.0234\t Average training accuracy 0.4639\n",
            "Epoch [2]\t Average validation loss 1.9647\t Average validation accuracy 0.5682\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.9620\t Accuracy 0.5800\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.9692\t Accuracy 0.5353\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.9676\t Accuracy 0.5267\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.9654\t Accuracy 0.5241\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.9630\t Accuracy 0.5256\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.9594\t Accuracy 0.5300\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.9555\t Accuracy 0.5321\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.9544\t Accuracy 0.5314\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.9504\t Accuracy 0.5363\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.9485\t Accuracy 0.5376\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.9459\t Accuracy 0.5406\n",
            "\n",
            "Epoch [3]\t Average training loss 1.9426\t Average training accuracy 0.5437\n",
            "Epoch [3]\t Average validation loss 1.8890\t Average validation accuracy 0.6298\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.8866\t Accuracy 0.6500\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.8981\t Accuracy 0.5910\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.8974\t Accuracy 0.5799\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.8966\t Accuracy 0.5774\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.8951\t Accuracy 0.5782\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.8921\t Accuracy 0.5816\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.8888\t Accuracy 0.5835\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.8889\t Accuracy 0.5818\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.8855\t Accuracy 0.5853\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.8845\t Accuracy 0.5855\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.8826\t Accuracy 0.5874\n",
            "\n",
            "Epoch [4]\t Average training loss 1.8798\t Average training accuracy 0.5897\n",
            "Epoch [4]\t Average validation loss 1.8299\t Average validation accuracy 0.6604\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.8282\t Accuracy 0.7100\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.8429\t Accuracy 0.6175\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.8430\t Accuracy 0.6077\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.8433\t Accuracy 0.6074\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.8424\t Accuracy 0.6086\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.8399\t Accuracy 0.6124\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.8371\t Accuracy 0.6135\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.8381\t Accuracy 0.6119\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.8352\t Accuracy 0.6144\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.8348\t Accuracy 0.6144\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.8335\t Accuracy 0.6156\n",
            "\n",
            "Epoch [5]\t Average training loss 1.8311\t Average training accuracy 0.6176\n",
            "Epoch [5]\t Average validation loss 1.7837\t Average validation accuracy 0.6808\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.7828\t Accuracy 0.7300\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.8000\t Accuracy 0.6373\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.8008\t Accuracy 0.6286\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.8020\t Accuracy 0.6267\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.8016\t Accuracy 0.6288\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.7994\t Accuracy 0.6321\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.7971\t Accuracy 0.6331\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.7987\t Accuracy 0.6314\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.7963\t Accuracy 0.6331\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.7964\t Accuracy 0.6331\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.7955\t Accuracy 0.6338\n",
            "\n",
            "Epoch [6]\t Average training loss 1.7934\t Average training accuracy 0.6356\n",
            "Epoch [6]\t Average validation loss 1.7477\t Average validation accuracy 0.6944\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.7479\t Accuracy 0.7300\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.7669\t Accuracy 0.6512\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.7683\t Accuracy 0.6431\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.7702\t Accuracy 0.6411\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.7702\t Accuracy 0.6418\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.7683\t Accuracy 0.6448\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.7662\t Accuracy 0.6455\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.7684\t Accuracy 0.6439\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.7663\t Accuracy 0.6453\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.7668\t Accuracy 0.6451\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.7662\t Accuracy 0.6455\n",
            "\n",
            "Epoch [7]\t Average training loss 1.7644\t Average training accuracy 0.6475\n",
            "Epoch [7]\t Average validation loss 1.7197\t Average validation accuracy 0.7028\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.7211\t Accuracy 0.7300\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.7415\t Accuracy 0.6614\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.7433\t Accuracy 0.6545\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.7459\t Accuracy 0.6516\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.7461\t Accuracy 0.6524\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.7445\t Accuracy 0.6547\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.7426\t Accuracy 0.6556\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.7452\t Accuracy 0.6536\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.7434\t Accuracy 0.6546\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.7442\t Accuracy 0.6543\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.7438\t Accuracy 0.6548\n",
            "\n",
            "Epoch [8]\t Average training loss 1.7422\t Average training accuracy 0.6565\n",
            "Epoch [8]\t Average validation loss 1.6983\t Average validation accuracy 0.7082\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.7008\t Accuracy 0.7200\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.7221\t Accuracy 0.6653\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.7244\t Accuracy 0.6600\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.7276\t Accuracy 0.6571\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.7279\t Accuracy 0.6579\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.7264\t Accuracy 0.6604\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.7248\t Accuracy 0.6613\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.7277\t Accuracy 0.6591\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.7261\t Accuracy 0.6599\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.7271\t Accuracy 0.6595\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.7269\t Accuracy 0.6602\n",
            "\n",
            "Epoch [9]\t Average training loss 1.7254\t Average training accuracy 0.6620\n",
            "Epoch [9]\t Average validation loss 1.6819\t Average validation accuracy 0.7150\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.6856\t Accuracy 0.7200\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.7075\t Accuracy 0.6720\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.7103\t Accuracy 0.6664\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.7139\t Accuracy 0.6630\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.7143\t Accuracy 0.6634\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.7130\t Accuracy 0.6651\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.7115\t Accuracy 0.6660\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.7146\t Accuracy 0.6638\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.7132\t Accuracy 0.6647\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.7144\t Accuracy 0.6642\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.7144\t Accuracy 0.6648\n",
            "\n",
            "Epoch [10]\t Average training loss 1.7129\t Average training accuracy 0.6665\n",
            "Epoch [10]\t Average validation loss 1.6696\t Average validation accuracy 0.7178\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.6745\t Accuracy 0.7300\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.6968\t Accuracy 0.6759\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.6999\t Accuracy 0.6701\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.7040\t Accuracy 0.6665\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.7044\t Accuracy 0.6665\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.7032\t Accuracy 0.6684\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.7019\t Accuracy 0.6691\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.7051\t Accuracy 0.6667\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.7038\t Accuracy 0.6675\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.7051\t Accuracy 0.6670\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.7052\t Accuracy 0.6675\n",
            "\n",
            "Epoch [11]\t Average training loss 1.7039\t Average training accuracy 0.6693\n",
            "Epoch [11]\t Average validation loss 1.6606\t Average validation accuracy 0.7206\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.6665\t Accuracy 0.7300\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.6891\t Accuracy 0.6788\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.6925\t Accuracy 0.6731\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.6969\t Accuracy 0.6690\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.6974\t Accuracy 0.6686\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.6963\t Accuracy 0.6703\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.6950\t Accuracy 0.6711\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.6984\t Accuracy 0.6688\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.6972\t Accuracy 0.6697\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.6986\t Accuracy 0.6690\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.6988\t Accuracy 0.6693\n",
            "\n",
            "Epoch [12]\t Average training loss 1.6975\t Average training accuracy 0.6713\n",
            "Epoch [12]\t Average validation loss 1.6541\t Average validation accuracy 0.7250\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.6611\t Accuracy 0.7300\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.6837\t Accuracy 0.6804\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.6875\t Accuracy 0.6749\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.6922\t Accuracy 0.6699\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.6926\t Accuracy 0.6693\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.6916\t Accuracy 0.6713\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.6904\t Accuracy 0.6722\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.6938\t Accuracy 0.6699\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.6927\t Accuracy 0.6706\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.6942\t Accuracy 0.6699\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.6944\t Accuracy 0.6702\n",
            "\n",
            "Epoch [13]\t Average training loss 1.6931\t Average training accuracy 0.6722\n",
            "Epoch [13]\t Average validation loss 1.6496\t Average validation accuracy 0.7262\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.6575\t Accuracy 0.7200\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.6802\t Accuracy 0.6824\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.6843\t Accuracy 0.6762\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.6893\t Accuracy 0.6707\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.6896\t Accuracy 0.6699\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.6886\t Accuracy 0.6720\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.6875\t Accuracy 0.6731\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.6910\t Accuracy 0.6708\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.6900\t Accuracy 0.6718\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.6915\t Accuracy 0.6710\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.6918\t Accuracy 0.6711\n",
            "\n",
            "Epoch [14]\t Average training loss 1.6905\t Average training accuracy 0.6732\n",
            "Epoch [14]\t Average validation loss 1.6467\t Average validation accuracy 0.7272\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.6555\t Accuracy 0.7200\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.6782\t Accuracy 0.6827\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.6825\t Accuracy 0.6766\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.6878\t Accuracy 0.6714\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.6880\t Accuracy 0.6704\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.6871\t Accuracy 0.6725\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.6860\t Accuracy 0.6732\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.6895\t Accuracy 0.6710\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.6885\t Accuracy 0.6719\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.6901\t Accuracy 0.6710\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.6904\t Accuracy 0.6711\n",
            "\n",
            "Epoch [15]\t Average training loss 1.6891\t Average training accuracy 0.6733\n",
            "Epoch [15]\t Average validation loss 1.6451\t Average validation accuracy 0.7282\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.6546\t Accuracy 0.7300\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.6774\t Accuracy 0.6837\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.6819\t Accuracy 0.6771\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.6874\t Accuracy 0.6721\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.6875\t Accuracy 0.6710\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.6866\t Accuracy 0.6735\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.6856\t Accuracy 0.6738\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.6890\t Accuracy 0.6713\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.6882\t Accuracy 0.6721\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.6897\t Accuracy 0.6712\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.6900\t Accuracy 0.6712\n",
            "\n",
            "Epoch [16]\t Average training loss 1.6888\t Average training accuracy 0.6735\n",
            "Epoch [16]\t Average validation loss 1.6444\t Average validation accuracy 0.7284\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.6547\t Accuracy 0.7200\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.6774\t Accuracy 0.6841\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.6822\t Accuracy 0.6763\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.6878\t Accuracy 0.6707\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.6879\t Accuracy 0.6699\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.6870\t Accuracy 0.6727\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.6860\t Accuracy 0.6732\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.6894\t Accuracy 0.6707\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.6886\t Accuracy 0.6712\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.6901\t Accuracy 0.6702\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.6905\t Accuracy 0.6703\n",
            "\n",
            "Epoch [17]\t Average training loss 1.6892\t Average training accuracy 0.6727\n",
            "Epoch [17]\t Average validation loss 1.6446\t Average validation accuracy 0.7274\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.6554\t Accuracy 0.7200\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.6782\t Accuracy 0.6837\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.6832\t Accuracy 0.6764\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.6889\t Accuracy 0.6707\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.6889\t Accuracy 0.6696\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.6880\t Accuracy 0.6724\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.6870\t Accuracy 0.6732\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.6904\t Accuracy 0.6706\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.6897\t Accuracy 0.6711\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.6912\t Accuracy 0.6700\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.6915\t Accuracy 0.6702\n",
            "\n",
            "Epoch [18]\t Average training loss 1.6902\t Average training accuracy 0.6725\n",
            "Epoch [18]\t Average validation loss 1.6453\t Average validation accuracy 0.7264\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.6567\t Accuracy 0.7100\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.6795\t Accuracy 0.6841\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.6847\t Accuracy 0.6773\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.6906\t Accuracy 0.6715\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.6904\t Accuracy 0.6697\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.6896\t Accuracy 0.6719\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.6886\t Accuracy 0.6728\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.6919\t Accuracy 0.6701\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.6912\t Accuracy 0.6705\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.6927\t Accuracy 0.6694\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.6931\t Accuracy 0.6693\n",
            "\n",
            "Epoch [19]\t Average training loss 1.6918\t Average training accuracy 0.6715\n",
            "Epoch [19]\t Average validation loss 1.6466\t Average validation accuracy 0.7242\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAPTQnthp_y-"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUw1D3fJp_y_",
        "outputId": "76dad2fc-95a3-43c3-d7bb-f7ebf3a4f034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.6903.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqWN0bGZp_zA"
      },
      "source": [
        "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
        "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUkIlxPbp_zB"
      },
      "source": [
        "reluMLP = Network()\n",
        "# Build ReLUMLP with FCLayer and ReLULayer\n",
        "# 128 is the number of hidden units, you can change by your own\n",
        "reluMLP.add(FCLayer(784, 128))\n",
        "reluMLP.add(ReLULayer())\n",
        "reluMLP.add(FCLayer(128, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uBP9w6_Ap_zC",
        "outputId": "aaa1d368-352e-4056-bf76-4ee229812887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5331\t Accuracy 0.0700\n",
            "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4470\t Accuracy 0.0624\n",
            "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3809\t Accuracy 0.0898\n",
            "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3178\t Accuracy 0.1193\n",
            "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2606\t Accuracy 0.1543\n",
            "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2138\t Accuracy 0.1859\n",
            "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.1622\t Accuracy 0.2189\n",
            "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1213\t Accuracy 0.2454\n",
            "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.0767\t Accuracy 0.2732\n",
            "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.0374\t Accuracy 0.2980\n",
            "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.9986\t Accuracy 0.3211\n",
            "\n",
            "Epoch [0]\t Average training loss 1.9610\t Average training accuracy 0.3428\n",
            "Epoch [0]\t Average validation loss 1.5023\t Average validation accuracy 0.6022\n",
            "\n",
            "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.4683\t Accuracy 0.6500\n",
            "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.5133\t Accuracy 0.5973\n",
            "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.4979\t Accuracy 0.5974\n",
            "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.4829\t Accuracy 0.5974\n",
            "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.4650\t Accuracy 0.6023\n",
            "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.4486\t Accuracy 0.6056\n",
            "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.4265\t Accuracy 0.6125\n",
            "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.4150\t Accuracy 0.6164\n",
            "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.3960\t Accuracy 0.6233\n",
            "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.3817\t Accuracy 0.6282\n",
            "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.3661\t Accuracy 0.6329\n",
            "\n",
            "Epoch [1]\t Average training loss 1.3495\t Average training accuracy 0.6391\n",
            "Epoch [1]\t Average validation loss 1.1067\t Average validation accuracy 0.7256\n",
            "\n",
            "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.0957\t Accuracy 0.7400\n",
            "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.1489\t Accuracy 0.6986\n",
            "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.1445\t Accuracy 0.6996\n",
            "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.1452\t Accuracy 0.6950\n",
            "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.1389\t Accuracy 0.6966\n",
            "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.1323\t Accuracy 0.6984\n",
            "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.1208\t Accuracy 0.7016\n",
            "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.1185\t Accuracy 0.7021\n",
            "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.1086\t Accuracy 0.7048\n",
            "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.1028\t Accuracy 0.7071\n",
            "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.0954\t Accuracy 0.7088\n",
            "\n",
            "Epoch [2]\t Average training loss 1.0863\t Average training accuracy 0.7122\n",
            "Epoch [2]\t Average validation loss 0.9179\t Average validation accuracy 0.7774\n",
            "\n",
            "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.9255\t Accuracy 0.7900\n",
            "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.9729\t Accuracy 0.7445\n",
            "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.9722\t Accuracy 0.7440\n",
            "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.9791\t Accuracy 0.7366\n",
            "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.9768\t Accuracy 0.7366\n",
            "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.9736\t Accuracy 0.7376\n",
            "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.9663\t Accuracy 0.7395\n",
            "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.9672\t Accuracy 0.7391\n",
            "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.9609\t Accuracy 0.7408\n",
            "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.9583\t Accuracy 0.7420\n",
            "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.9542\t Accuracy 0.7428\n",
            "\n",
            "Epoch [3]\t Average training loss 0.9482\t Average training accuracy 0.7449\n",
            "Epoch [3]\t Average validation loss 0.8093\t Average validation accuracy 0.8054\n",
            "\n",
            "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8291\t Accuracy 0.8000\n",
            "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8698\t Accuracy 0.7684\n",
            "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8708\t Accuracy 0.7674\n",
            "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8808\t Accuracy 0.7601\n",
            "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8801\t Accuracy 0.7600\n",
            "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8783\t Accuracy 0.7599\n",
            "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8733\t Accuracy 0.7613\n",
            "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8755\t Accuracy 0.7610\n",
            "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8709\t Accuracy 0.7623\n",
            "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8698\t Accuracy 0.7626\n",
            "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8674\t Accuracy 0.7628\n",
            "\n",
            "Epoch [4]\t Average training loss 0.8629\t Average training accuracy 0.7644\n",
            "Epoch [4]\t Average validation loss 0.7383\t Average validation accuracy 0.8210\n",
            "\n",
            "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.7667\t Accuracy 0.8100\n",
            "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8014\t Accuracy 0.7871\n",
            "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8034\t Accuracy 0.7852\n",
            "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8152\t Accuracy 0.7770\n",
            "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8152\t Accuracy 0.7766\n",
            "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8141\t Accuracy 0.7763\n",
            "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8105\t Accuracy 0.7773\n",
            "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8133\t Accuracy 0.7767\n",
            "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8097\t Accuracy 0.7777\n",
            "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8094\t Accuracy 0.7776\n",
            "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8080\t Accuracy 0.7776\n",
            "\n",
            "Epoch [5]\t Average training loss 0.8044\t Average training accuracy 0.7790\n",
            "Epoch [5]\t Average validation loss 0.6881\t Average validation accuracy 0.8320\n",
            "\n",
            "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.7225\t Accuracy 0.8100\n",
            "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.7521\t Accuracy 0.7986\n",
            "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.7548\t Accuracy 0.7967\n",
            "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.7678\t Accuracy 0.7876\n",
            "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.7681\t Accuracy 0.7877\n",
            "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.7675\t Accuracy 0.7872\n",
            "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.7648\t Accuracy 0.7879\n",
            "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.7679\t Accuracy 0.7874\n",
            "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.7650\t Accuracy 0.7885\n",
            "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.7651\t Accuracy 0.7883\n",
            "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.7643\t Accuracy 0.7881\n",
            "\n",
            "Epoch [6]\t Average training loss 0.7613\t Average training accuracy 0.7891\n",
            "Epoch [6]\t Average validation loss 0.6504\t Average validation accuracy 0.8402\n",
            "\n",
            "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.6893\t Accuracy 0.8100\n",
            "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.7146\t Accuracy 0.8061\n",
            "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.7179\t Accuracy 0.8048\n",
            "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.7317\t Accuracy 0.7966\n",
            "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.7321\t Accuracy 0.7967\n",
            "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.7317\t Accuracy 0.7965\n",
            "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.7297\t Accuracy 0.7970\n",
            "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.7330\t Accuracy 0.7965\n",
            "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.7305\t Accuracy 0.7970\n",
            "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.7309\t Accuracy 0.7965\n",
            "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.7306\t Accuracy 0.7959\n",
            "\n",
            "Epoch [7]\t Average training loss 0.7280\t Average training accuracy 0.7968\n",
            "Epoch [7]\t Average validation loss 0.6209\t Average validation accuracy 0.8466\n",
            "\n",
            "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.6631\t Accuracy 0.8200\n",
            "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.6849\t Accuracy 0.8129\n",
            "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.6886\t Accuracy 0.8108\n",
            "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.7030\t Accuracy 0.8029\n",
            "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.7035\t Accuracy 0.8037\n",
            "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.7032\t Accuracy 0.8038\n",
            "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.7018\t Accuracy 0.8038\n",
            "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.7051\t Accuracy 0.8032\n",
            "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.7030\t Accuracy 0.8035\n",
            "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.7036\t Accuracy 0.8031\n",
            "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.7035\t Accuracy 0.8025\n",
            "\n",
            "Epoch [8]\t Average training loss 0.7013\t Average training accuracy 0.8032\n",
            "Epoch [8]\t Average validation loss 0.5970\t Average validation accuracy 0.8522\n",
            "\n",
            "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.6416\t Accuracy 0.8300\n",
            "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.6605\t Accuracy 0.8184\n",
            "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.6647\t Accuracy 0.8165\n",
            "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.6795\t Accuracy 0.8087\n",
            "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.6800\t Accuracy 0.8100\n",
            "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.6798\t Accuracy 0.8098\n",
            "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.6788\t Accuracy 0.8094\n",
            "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.6822\t Accuracy 0.8088\n",
            "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.6803\t Accuracy 0.8092\n",
            "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.6810\t Accuracy 0.8087\n",
            "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.6813\t Accuracy 0.8080\n",
            "\n",
            "Epoch [9]\t Average training loss 0.6793\t Average training accuracy 0.8086\n",
            "Epoch [9]\t Average validation loss 0.5772\t Average validation accuracy 0.8550\n",
            "\n",
            "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.6236\t Accuracy 0.8300\n",
            "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.6402\t Accuracy 0.8233\n",
            "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.6448\t Accuracy 0.8218\n",
            "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.6599\t Accuracy 0.8136\n",
            "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.6604\t Accuracy 0.8151\n",
            "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.6602\t Accuracy 0.8146\n",
            "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.6596\t Accuracy 0.8142\n",
            "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.6629\t Accuracy 0.8136\n",
            "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.6613\t Accuracy 0.8139\n",
            "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.6621\t Accuracy 0.8135\n",
            "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.6625\t Accuracy 0.8127\n",
            "\n",
            "Epoch [10]\t Average training loss 0.6607\t Average training accuracy 0.8133\n",
            "Epoch [10]\t Average validation loss 0.5605\t Average validation accuracy 0.8586\n",
            "\n",
            "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.6083\t Accuracy 0.8300\n",
            "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.6228\t Accuracy 0.8284\n",
            "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.6278\t Accuracy 0.8259\n",
            "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.6432\t Accuracy 0.8178\n",
            "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.6436\t Accuracy 0.8193\n",
            "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.6435\t Accuracy 0.8190\n",
            "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.6431\t Accuracy 0.8183\n",
            "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.6464\t Accuracy 0.8176\n",
            "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.6449\t Accuracy 0.8177\n",
            "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.6458\t Accuracy 0.8173\n",
            "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.6464\t Accuracy 0.8166\n",
            "\n",
            "Epoch [11]\t Average training loss 0.6448\t Average training accuracy 0.8172\n",
            "Epoch [11]\t Average validation loss 0.5462\t Average validation accuracy 0.8616\n",
            "\n",
            "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.5949\t Accuracy 0.8300\n",
            "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.6078\t Accuracy 0.8327\n",
            "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.6131\t Accuracy 0.8301\n",
            "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.6287\t Accuracy 0.8217\n",
            "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.6291\t Accuracy 0.8228\n",
            "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.6289\t Accuracy 0.8223\n",
            "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.6288\t Accuracy 0.8216\n",
            "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.6321\t Accuracy 0.8208\n",
            "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.6308\t Accuracy 0.8207\n",
            "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.6317\t Accuracy 0.8202\n",
            "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.6324\t Accuracy 0.8195\n",
            "\n",
            "Epoch [12]\t Average training loss 0.6310\t Average training accuracy 0.8201\n",
            "Epoch [12]\t Average validation loss 0.5337\t Average validation accuracy 0.8626\n",
            "\n",
            "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.5831\t Accuracy 0.8400\n",
            "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.5946\t Accuracy 0.8357\n",
            "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.6003\t Accuracy 0.8333\n",
            "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.6160\t Accuracy 0.8250\n",
            "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.6163\t Accuracy 0.8259\n",
            "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.6162\t Accuracy 0.8251\n",
            "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.6163\t Accuracy 0.8246\n",
            "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.6196\t Accuracy 0.8237\n",
            "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.6183\t Accuracy 0.8237\n",
            "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.6194\t Accuracy 0.8231\n",
            "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.6201\t Accuracy 0.8223\n",
            "\n",
            "Epoch [13]\t Average training loss 0.6188\t Average training accuracy 0.8228\n",
            "Epoch [13]\t Average validation loss 0.5227\t Average validation accuracy 0.8636\n",
            "\n",
            "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.5726\t Accuracy 0.8300\n",
            "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.5829\t Accuracy 0.8369\n",
            "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.5890\t Accuracy 0.8351\n",
            "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.6048\t Accuracy 0.8277\n",
            "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.6050\t Accuracy 0.8288\n",
            "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.6049\t Accuracy 0.8279\n",
            "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.6052\t Accuracy 0.8271\n",
            "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.6084\t Accuracy 0.8262\n",
            "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.6073\t Accuracy 0.8264\n",
            "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.6083\t Accuracy 0.8257\n",
            "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.6092\t Accuracy 0.8249\n",
            "\n",
            "Epoch [14]\t Average training loss 0.6080\t Average training accuracy 0.8253\n",
            "Epoch [14]\t Average validation loss 0.5130\t Average validation accuracy 0.8656\n",
            "\n",
            "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.5632\t Accuracy 0.8400\n",
            "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.5725\t Accuracy 0.8396\n",
            "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.5789\t Accuracy 0.8374\n",
            "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.5947\t Accuracy 0.8299\n",
            "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.5949\t Accuracy 0.8311\n",
            "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.5948\t Accuracy 0.8301\n",
            "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.5952\t Accuracy 0.8292\n",
            "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.5984\t Accuracy 0.8283\n",
            "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.5974\t Accuracy 0.8286\n",
            "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.5985\t Accuracy 0.8280\n",
            "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.5994\t Accuracy 0.8272\n",
            "\n",
            "Epoch [15]\t Average training loss 0.5983\t Average training accuracy 0.8277\n",
            "Epoch [15]\t Average validation loss 0.5043\t Average validation accuracy 0.8668\n",
            "\n",
            "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.5546\t Accuracy 0.8500\n",
            "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.5631\t Accuracy 0.8420\n",
            "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.5698\t Accuracy 0.8393\n",
            "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.5857\t Accuracy 0.8317\n",
            "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.5858\t Accuracy 0.8327\n",
            "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.5856\t Accuracy 0.8317\n",
            "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.5862\t Accuracy 0.8311\n",
            "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.5894\t Accuracy 0.8300\n",
            "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.5885\t Accuracy 0.8302\n",
            "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.5896\t Accuracy 0.8297\n",
            "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.5906\t Accuracy 0.8291\n",
            "\n",
            "Epoch [16]\t Average training loss 0.5895\t Average training accuracy 0.8294\n",
            "Epoch [16]\t Average validation loss 0.4964\t Average validation accuracy 0.8692\n",
            "\n",
            "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.5468\t Accuracy 0.8500\n",
            "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.5545\t Accuracy 0.8437\n",
            "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.5615\t Accuracy 0.8418\n",
            "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.5775\t Accuracy 0.8339\n",
            "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.5776\t Accuracy 0.8348\n",
            "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.5774\t Accuracy 0.8340\n",
            "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.5781\t Accuracy 0.8333\n",
            "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.5812\t Accuracy 0.8322\n",
            "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.5804\t Accuracy 0.8324\n",
            "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.5815\t Accuracy 0.8318\n",
            "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.5826\t Accuracy 0.8311\n",
            "\n",
            "Epoch [17]\t Average training loss 0.5816\t Average training accuracy 0.8314\n",
            "Epoch [17]\t Average validation loss 0.4892\t Average validation accuracy 0.8700\n",
            "\n",
            "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.5396\t Accuracy 0.8500\n",
            "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.5467\t Accuracy 0.8475\n",
            "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.5540\t Accuracy 0.8445\n",
            "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.5700\t Accuracy 0.8361\n",
            "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.5700\t Accuracy 0.8366\n",
            "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.5698\t Accuracy 0.8359\n",
            "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.5707\t Accuracy 0.8351\n",
            "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.5737\t Accuracy 0.8341\n",
            "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.5729\t Accuracy 0.8343\n",
            "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.5741\t Accuracy 0.8338\n",
            "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.5752\t Accuracy 0.8331\n",
            "\n",
            "Epoch [18]\t Average training loss 0.5743\t Average training accuracy 0.8334\n",
            "Epoch [18]\t Average validation loss 0.4827\t Average validation accuracy 0.8708\n",
            "\n",
            "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.5330\t Accuracy 0.8500\n",
            "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.5396\t Accuracy 0.8480\n",
            "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.5472\t Accuracy 0.8452\n",
            "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.5632\t Accuracy 0.8371\n",
            "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.5631\t Accuracy 0.8378\n",
            "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.5629\t Accuracy 0.8370\n",
            "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.5638\t Accuracy 0.8364\n",
            "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.5668\t Accuracy 0.8355\n",
            "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.5661\t Accuracy 0.8358\n",
            "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.5673\t Accuracy 0.8353\n",
            "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.5685\t Accuracy 0.8347\n",
            "\n",
            "Epoch [19]\t Average training loss 0.5676\t Average training accuracy 0.8350\n",
            "Epoch [19]\t Average validation loss 0.4767\t Average validation accuracy 0.8702\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIlqf5Ejp_zE",
        "outputId": "9bb8d187-e65c-4eb4-b80f-d6ac0f7e347b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "The test accuracy is 0.8462.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dho-ENHXp_zF"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VSBMbwHcp62",
        "outputId": "ed22a65b-cbb4-45ea-fc34-6e22ad0d5074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(sigmoid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.1866350846503138, 2.0617011136512464, 1.9646944255363143, 1.8890176250781499, 1.8298651627831157, 1.7836528978056745, 1.7476515635936019, 1.7197463183704778, 1.6982760158083265, 1.681923388536739, 1.6696378943105494, 1.6605796598419575, 1.65407728458952, 1.6495950461811222, 1.646706781583974, 1.6450747548143179, 1.6444324190056157, 1.6445703175700483, 1.6453245622714494, 1.6465674439797158]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4aBQbJp_zG",
        "outputId": "f4b566c6-850a-491e-b4b7-4c33c4a52f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
        "                   'relu': [relu_loss, relu_acc]})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU0iGrv-p_zI"
      },
      "source": [
        "### ~~You have finished homework1-mlp, congratulations!~~  \n",
        "\n",
        "**Next, according to the requirements 4) of report:**\n",
        "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
        "\n",
        "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
      ]
    }
  ]
}